{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "!pip install beautifulsoup4 #install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install nltk #install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "#matplotlib.rcParams['backend'] = \"GtkAgg\"\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking only Sentiments as Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments = sc.textFile(\"./data/train.csv\").map(lambda line: line.split(\",\",2)[0].strip(\"\\\"\")).map(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiments_test = sc.textFile(\"./data/test.csv\").map(lambda line: line.split(\",\",2)[0].strip(\"\\\"\")).map(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "Below is the Text Processing Function taken from our Data Mining Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def ProcessTweet(Tweet):\n",
    "    try:\n",
    "        #Tweet = Tweet.encode('utf-8')\n",
    "        stemmer = PorterStemmer()\n",
    "        print (Tweet)\n",
    "        Tweet = Tweet.lower()\n",
    "        soup = BeautifulSoup(Tweet, \"html.parser\")\n",
    "        eTags= soup.find_all('e')\n",
    "        for tag in eTags:\n",
    "            tag.extract()\n",
    "        #Tweet = soup.encode('ascii')\n",
    "        Tweet = re.sub(r'pic.twitter.*?$', '', Tweet)\n",
    "        Tweet = re.sub(r'pic.twitter.*? ', '', Tweet)\n",
    "        Tweet = re.sub(r'(.)\\1{2,}', r'\\1\\1', Tweet)\n",
    "        Tweet = re.sub('((www\\.[^\\s]+)|(http://[^\\s]+))','',Tweet)  #remove url\n",
    "        Tweet = re.sub(r'#([^\\s])*$', '', Tweet)\n",
    "        Tweet = re.sub(r'#([^\\s])* ', '', Tweet)\n",
    "        Tweet = re.sub(r'\\@([^\\s])*$', '', Tweet)\n",
    "        Tweet = re.sub(r'\\@([^\\s])* ', '', Tweet)\n",
    "        Tweet = re.sub(\"\\d\",'',Tweet)                               #remove digits\n",
    "        Tweet = re.sub(\"[!\\'.\\\"%/*$;:\\(\\):,?]\",'',Tweet)                        #remove special characters\n",
    "        Tweet = re.sub(r'\\-',' ',Tweet)                             #replace - with white space\n",
    "        Tweet = re.sub(r'\\'m', ' am', Tweet)\n",
    "        Tweet = re.sub(r'\\'d', ' would', Tweet)\n",
    "        Tweet = re.sub(r'\\'ll', ' will', Tweet)\n",
    "        Tweet = re.sub(r'\\&', 'and', Tweet)\n",
    "        Tweet = re.sub(r'\\b\\w{1,2}\\b','', Tweet).strip()            #remove 2 or lesser length words\n",
    "        Tweet = re.sub('[\\s]+', ' ', Tweet)\n",
    "        Tweet = re.sub('<>', '', Tweet)\n",
    "        print (Tweet)\n",
    "        #remove trailing duplicate characters\n",
    "        words=[]\n",
    "        for word in Tweet.split():\n",
    "            l=word[-1]\n",
    "            word = word.rstrip(word[-1])\n",
    "            word += l\n",
    "            words.append(word)\n",
    "        Tweet = ' '.join(words)\n",
    "        print (Tweet)\n",
    "        #stemming\n",
    "        words=[]\n",
    "        for word in Tweet.split():\n",
    "            data = stemmer.stem(word)\n",
    "            words.append(str(data))\n",
    "        Tweet = ' '.join(words)\n",
    "        print (Tweet)\n",
    "        #stop word removal\n",
    "        stop = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', \n",
    "                'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "                'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "                'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "                'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "                'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "                'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', \n",
    "                'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
    "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "        final = [i for i in Tweet.split() if i not in stop]\n",
    "        tweet = ' '.join(final)\n",
    "        print (tweet)\n",
    "        return tweet\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending only the Tweets to the Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = sc.textFile(\"./data/train.csv\",).map(lambda line: line.split(\",\",5)[-1]).map(ProcessTweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_test = sc.textFile(\"./data/test.csv\",).map(lambda line: line.split(\",\",5)[-1]).map(ProcessTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Tweets after Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = tweets.map(lambda line: line.split())\n",
    "tweets.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_test = tweets_test.map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Features using the mllib package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF1 = HashingTF()\n",
    "tf_test = hashingTF1.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import IDF\n",
    "idf = IDF(minDocFreq=0).fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Transforming Training and Test data features to tf-idf. For test data, IDF of train data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_test = idf.transform(tf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating Sentiments and Features as labeled point for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = sentiments.zip(tfidf).map(lambda x: LabeledPoint(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = sentiments_test.zip(tfidf_test).map(lambda x: LabeledPoint(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run All Cells above this one. From here on, use the cell you want to exectue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive bayes Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = NaiveBayes.train(training)\n",
    "\n",
    "#Can change the 'test' to 'train' to predict accordingly\n",
    "labelsAndPred = test.map(lambda p: (float(model.predict(p.features)),p.label))\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "#metrics object instantiation\n",
    "metrics = MulticlassMetrics(labelsAndPred)\n",
    "metrics2  = BinaryClassificationMetrics(labelsAndPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating metrics from the confusion matrix\n",
    "x = metrics.confusionMatrix().toArray()\n",
    "trueP=x[1][1] \n",
    "trueN=x[0][0]\n",
    "falseP=x[0][1]\n",
    "falseN=x[1][0]\n",
    "NBprecision= trueP/(trueP+falseP)\n",
    "NBrecall= trueP/(trueP+falseN)\n",
    "NBf1score= (2*NBprecision*NBrecall)/(NBprecision+NBrecall)\n",
    "NBaccuracy = (trueP+trueN)/(trueP+trueN+falseP+falseN)\n",
    "print (NBprecision,NBrecall,NBf1score,NBaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-fold Cross Validation for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tprecision= 0\n",
    "#trecall= 0\n",
    "#tf1score1= 0\n",
    "taccuracy = 0\n",
    "for i in range(10):\n",
    "    \n",
    "    trainrdd, testrdd = training.randomSplit([0.9,0.1], 2*i*i)\n",
    "    model = NaiveBayes.train(trainrdd)\n",
    "    labelsAndPred = testrdd.map(lambda p: (float(model.predict(p.features)),p.label))\n",
    "    metrics = MulticlassMetrics(labelsAndPred)\n",
    "    x = metrics.confusionMatrix().toArray()\n",
    "    trueP=x[1][1]\n",
    "    trueN=x[0][0]\n",
    "    falseP=x[0][1]\n",
    "    falseN=x[1][0]\n",
    "    #tprecision+= trueP/(trueP+falseP)\n",
    "    #trecall+= trueP/(trueP+falseN)\n",
    "    #tf1score1+= (2*tprecision*trecall)/(tprecision+trecall)\n",
    "    taccuracy+= (trueP+trueN)/(trueP+trueN+falseP+falseN)\n",
    "    print(taccuracy)\n",
    "accuracy = taccuracy/10\n",
    "print(\"Final Accuracy for K-Cross Naive bayes is: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to computer ROC for NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tpr = []\n",
    "fpr = []\n",
    "for lm in range(11):\n",
    "    model = NaiveBayes.train(training, lambda_=lm/10.0)\n",
    "    labelsAndPred = test.map(lambda p: (float(model.predict(p.features)),p.label))\n",
    "    from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "    metrics = MulticlassMetrics(labelsAndPred)\n",
    "    metrics2  = BinaryClassificationMetrics(labelsAndPred)\n",
    "    x = metrics.confusionMatrix().toArray()\n",
    "    trueP=x[1][1] \n",
    "    trueN=x[0][0]\n",
    "    falseP=x[0][1]\n",
    "    falseN=x[1][0]\n",
    "    NBrecall= trueP/(trueP+falseN)\n",
    "    tpr.append(NBrecall)\n",
    "    NBfpr= falseP/(falseP+trueN)\n",
    "    fpr.append(NBfpr)\n",
    "    print(tpr,fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (metrics2.areaUnderROC)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "model = LogisticRegressionWithLBFGS.train(training,numClasses=2)\n",
    "#Can change the 'test' to 'train' to predict accordingly\n",
    "labelsAndPred = test.map(lambda p: (float(model.predict(p.features)),p.label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculating metrics from the confusion matrix\n",
    "metrics = MulticlassMetrics(labelsAndPred)\n",
    "x = metrics.confusionMatrix().toArray()\n",
    "trueP=x[1][1]\n",
    "trueN=x[0][0]\n",
    "falseP=x[0][1]\n",
    "falseN=x[1][0]\n",
    "logprecision= trueP/(trueP+falseP)\n",
    "logrecall= trueP/(trueP+falseN)\n",
    "logfpr=\n",
    "logf1score= (2*logprecision*logrecall)/(logprecision+logrecall)\n",
    "logaccuracy = (trueP+trueN)/(trueP+trueN+falseP+falseN)\n",
    "print (logprecision,logrecall,logf1score,logaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the ROC by changing the threshold values for LOG from 0.0-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionWithLBFGS.train(training,numClasses=2)\n",
    "tpr = []\n",
    "fpr = []\n",
    "for j in range(11):    \n",
    "    model.setThreshold(j/10.0)\n",
    "    labelsAndPred = test.map(lambda p: (float(model.predict(p.features)),p.label))\n",
    "    metrics = MulticlassMetrics(labelsAndPred)\n",
    "    x = metrics.confusionMatrix().toArray()\n",
    "    trueP=x[1][1]\n",
    "    trueN=x[0][0]\n",
    "    falseP=x[0][1]\n",
    "    falseN=x[1][0]\n",
    "    logrecall= trueP/(trueP+falseN)\n",
    "    tpr.append(logrecall)\n",
    "    logfpr= falseP/(falseP+trueN)\n",
    "    fpr.append(logfpr)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting The ROC for LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "plt2.clf()\n",
    "plt2.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f)' % (roc_auc))\n",
    "plt2.xlim([-0.05, 1.05])\n",
    "plt2.ylim([-0.05, 1.05])\n",
    "plt2.xlabel('False Positive Rate')\n",
    "plt2.ylabel('True Positive Rate')\n",
    "plt2.title('Receiver operating characteristic')\n",
    "plt2.legend(loc=\"lower right\")\n",
    "plt2.show()\n",
    "plt2.savefig('ROC-LOG.png') #savefig in home directory if .show() does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10-fold Cross Validation for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tprecision= 0\n",
    "#trecall= 0\n",
    "#tf1score1= 0\n",
    "taccuracy = 0\n",
    "for i in range(10):\n",
    "    \n",
    "    trainrdd, testrdd = training.randomSplit([0.9,0.1], 2*i*i)\n",
    "    model = LogisticRegressionWithLBFGS.train(trainrdd,numClasses=2)\n",
    "    labelsAndPred = testrdd.map(lambda p: (float(model.predict(p.features)),p.label))\n",
    "    metrics = MulticlassMetrics(labelsAndPred)\n",
    "    x = metrics.confusionMatrix().toArray()\n",
    "    trueP=x[1][1]\n",
    "    trueN=x[0][0]\n",
    "    falseP=x[0][1]\n",
    "    falseN=x[1][0]\n",
    "    #tprecision+= trueP/(trueP+falseP)\n",
    "    #trecall+= trueP/(trueP+falseN)\n",
    "    #tf1score1+= (2*tprecision*trecall)/(tprecision+trecall)\n",
    "    taccuracy+= (trueP+trueN)/(trueP+trueN+falseP+falseN)\n",
    "    print(taccuracy)\n",
    "accuracy = taccuracy/10\n",
    "print(\"Final Accuracy for K-Cross Logistic Regression is: \",accuracy)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree,DecisionTreeModel\n",
    "model = DecisionTree.trainClassifier(test,numClasses=2, categoricalFeaturesInfo={},maxDepth=5,maxBins=10)\n",
    "#Can change the 'test' to 'train' to predict accordingly\n",
    "labelsAndPred = test.map(lambda p: (float(model.predict(p.features)),p.label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating metrics from the confusion matrix\n",
    "metrics = MulticlassMetrics(labelsAndPred)\n",
    "x = metrics.confusionMatrix().toArray()\n",
    "trueP=x[1][1]\n",
    "trueN=x[0][0]\n",
    "falseP=x[0][1]\n",
    "falseN=x[1][0]\n",
    "dtprecision= trueP/(trueP+falseP)\n",
    "dtgrecall= trueP/(trueP+falseN)\n",
    "dtf1score= (2*dtprecision*dtrecall)/(dtprecision+dtrecall)\n",
    "dtaccuracy = (trueP+trueN)/(trueP+trueN+falseP+falseN)\n",
    "print (dtprecision,dtrecall,dtf1score,dtaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "10-fold Cross Validation for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tprecision= 0\n",
    "#trecall= 0\n",
    "#tf1score1= 0\n",
    "taccuracy = 0\n",
    "for i in range(10):\n",
    "    \n",
    "    trainrdd, testrdd = training.randomSplit([0.9,0.1], 2*i*i)\n",
    "    model = DecisionTree.trainClassifier(test,numClasses=2, categoricalFeaturesInfo={},maxDepth=5,maxBins=10)\n",
    "    labelsAndPred = testrdd.map(lambda p: (float(model.predict(p.features)),p.label))\n",
    "    metrics = MulticlassMetrics(labelsAndPred)\n",
    "    x = metrics.confusionMatrix().toArray()\n",
    "    trueP=x[1][1]\n",
    "    trueN=x[0][0]\n",
    "    falseP=x[0][1]\n",
    "    falseN=x[1][0]\n",
    "    #tprecision+= trueP/(trueP+falseP)\n",
    "    #trecall+= trueP/(trueP+falseN)\n",
    "    #tf1score1+= (2*tprecision*trecall)/(tprecision+trecall)\n",
    "    taccuracy+= (trueP+trueN)/(trueP+trueN+falseP+falseN)\n",
    "    print(taccuracy)\n",
    "accuracy = taccuracy/10\n",
    "print(\"Final Accuracy for K-Cross Logistic Regression is: \",accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf() \n",
    "plt.plot([0.849,0.718,0.701],label='Naive Bayes') #Plotting Naive Bayes Accuracies\n",
    "plt.plot([0.886,0.725,0.775],label='Logistic Regression') #Plotting Logistic regression Accuracies\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#Plot with Training accracy at 0.0, 10fold accuracy at 1.0 and Test accuracy at 2.0 on the X-axis\n",
    "plt.xlabel(\"Training                                     10fold                                         Test\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('Accuracies.png') #savefig in home directory if .show() does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
